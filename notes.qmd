---
title: "APH101--Biostatistics and R Notes"
format: html
editor: visual
---

# CLT toy exampels

```{r}
library(ggplot2)
library(gridExtra)
```

```{r}
set.seed(003)
r <- 1000

data <- rnorm(n = 10000, mean = 10, sd = 2)

sample_means_n5 <- c()
for (i in 1:r){
  sample_means_n5[i] <- mean(sample(data, 5))
}

sample_means_n30 <- c()
for (i in 1:r){
  sample_means_n30[i] <- mean(sample(data, 30))
}

sample_means_n1000 <- c()
for (i in 1:r){
  sample_means_n1000[i] <- mean(sample(data, 1000))
}

par(mfcol = c(2, 2))
hist(data, main='Population')
hist(sample_means_n30, main='Sample Means (n=30)', xlim=c(9, 11))
hist(sample_means_n5, main='Sample Means (n=5)', xlim=c(9, 11))
hist(sample_means_n1000, main='Sample Means (n=1000)', xlim=c(9, 11))
```

```{r}
set.seed(123)
r <- 1000

data <- runif(n = 10000, min = 2, max = 6)

sample_means_n5 <- c()
for (i in 1:r){
  sample_means_n5[i] <- mean(sample(data, 5))
}

sample_means_n30 <- c()
for (i in 1:r){
  sample_means_n30[i] <- mean(sample(data, 30))
}

sample_means_n1000 <- c()
for (i in 1:r){
  sample_means_n1000[i] <- mean(sample(data, 1000))
}

par(mfcol = c(2, 2))
hist(data, main='Population')
hist(sample_means_n30, main='Sample Means (n=30)')
hist(sample_means_n5, main='Sample Means (n=5)')
hist(sample_means_n1000, main='Sample Means (n=1000)')
```

```{r}
set.seed(123)
r <- 1000

data <- rpois(n = 10000, lambda = 5)

sample_means_n5 <- c()
for (i in 1:r){
  sample_means_n5[i] <- mean(sample(data, 5))
}

sample_means_n30 <- c()
for (i in 1:r){
  sample_means_n30[i] <- mean(sample(data, 30))
}

sample_means_n1000 <- c()
for (i in 1:r){
  sample_means_n1000[i] <- mean(sample(data, 1000))
}

par(mfcol = c(2, 2))
hist(data, main='Population')
hist(sample_means_n30, main='Sample Means (n=30)')
hist(sample_means_n5, main='Sample Means (n=5)')
hist(sample_means_n1000, main='Sample Means (n=1000)')
```

```{r}
set.seed(003)

rprobs = sample(1:5, 5)
rprobs = rep(16*rprobs, each=20)
rprobs = rprobs + 20*runif(100)
rprobs = rprobs / sum(rprobs)
data = sample(1:100, size=10000, replace=TRUE, prob=rprobs)

sample_means_n5 <- c()
for (i in 1:r){
  sample_means_n5[i] <- mean(sample(data, 5))
}

sample_means_n30 <- c()
for (i in 1:r){
  sample_means_n30[i] <- mean(sample(data, 30))
}

sample_means_n1000 <- c()
for (i in 1:r){
  sample_means_n1000[i] <- mean(sample(data, 1000))
}

par(mfcol = c(2, 2))
hist(data, main='Population')
hist(sample_means_n30, main='Sample Means (n=30)')
hist(sample_means_n5, main='Sample Means (n=5)')
hist(sample_means_n1000, main='Sample Means (n=1000)')
```

# CI

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä»Žæ€»ä½“ä¸­æŠ½å–ä¸€ä¸ªæ ·æœ¬ï¼Œå¹¶è®¡ç®—æ ·æœ¬å‡å€¼ã€‚ é‚£å°±æ˜¯æ ¹æ®ä¸€ä¸ªæ ·æœ¬å‡å€¼ä¸€ä¸ªç½®ä¿¡åŒºé—´ï¼Œæ‰€ä»¥ä¸€ä¸ªç½®ä¿¡åŒºé—´åªæœ‰åŒ…å«æˆ–è€…ä¸åŒ…å«ä¸¤ç§æƒ…å†µï¼ˆå’Œè´å¶æ–¯çš„æ˜¯random variableä¸ä¸€æ ·ï¼‰ é‚£æ¯ä¸ªç½®ä¿¡åŒºé—´éƒ½ä¸ä¸€æ · ä½†æˆ‘ä»¬æž„é€ çš„æ˜¯95%CI ï¼ˆæœ‰95%çš„confidence intervalä¼šåŒ…å«--å“ªé‡Œä½“çŽ°äº†ï¼‰ï¼Œä¸åŒ…å«çš„è¯å¾ˆå¯èƒ½æœ‰é—®é¢˜ï¼Œå¼•å‡ºå‡è®¾æ£€éªŒå¦‚æžœå‡è®¾äº†å‡å€¼æ˜¯xxï¼Œç„¶åŽæ ¹æ®æ ·æœ¬å¾—åˆ°çš„ç½®ä¿¡åŒºé—´ä¸åŒ…å«ï¼Œè¯´æ˜Žæœ‰é—®é¢˜ï¼Œæ‹’ç»å®ƒã€‚

## ? use CLT to construct an interval

## Intro

-   Even the most efficient unbiased estimator is unlikely to estimate population parameter exactly.

-   If we were to construct a 95% confidence interval for some population characteristics (population proportion p or population mean $\mu$), we would be using a method that is successful 95% of the time.

-   This is also about the question relavent to "How to choose a sample size"

## margin of error

For a given level of confidence c, The greatest possible distance between the point estimate and the value of the parameter it is estimating.

Sometimes called the maximum error of estimate or error tolerance.

## Interpretation

For confidence level of 95%, t for any normal distribution: About 95% of the values are within 1.96 standard deviations of the mean. (Recall the concept of Z-scores)

That is, if this method was used to generate an interval estimate over and over again with different samples, in the long run 95% of the resulting intervals would include the actual value of the characteristic being estimated.

-   The confidence level 95% refers to the method used to construct the interval rather than to any particular interval, such as the one we obtained.

![](images/clipboard-1141088211.png)

### eg

#### Proportion

-   check to make sure that the three necessary conditions are met:

$n\hat p \ge 10, n(1-\hat p) \ge 10$

$\hat p= (\hat p -  , \hat p + 1.96)$

(from : $\frac {\hat p -p }{\sigma/\sqrt n} = 1.96$)

![](images/clipboard-3637074733.png)

-   sample size question

using sample proportion with 95% confidence interval, and we want ME no more than 5%, we may be interested in solving n from the following equation:

0.05=1.96 $\sqrt {\frac {\hat p(1-\hat p) }{ n}}$

#### Mean

-   $\sigma$ known

n $\ge 30$

-   $\sigma$ unknown --- Use s as the estimate for Ïƒ (t-statistics with df )

Assumption: The population distribution is normal. (Remark:This assumption is not critical if the sample size is large, but it is important when the sample size is small.)

(t-statistics with df=n-1)

$t = \bar x...$

# Hypothesis testing

-   opening: does these data (gained sleep hour) means the low dose of drugs improves the length of people's sleep?

-   Suppose $X_1,...X_n \sim^{iid}$ N$(\mu,\sigma^2)$ with both of 2 parameters unknown

    -   let $t_{obs}=t(x)=\frac{\bar x - \mu _0}{s/\sqrt n}$

# Chi

-   Standard normal

-   the square sum of standard normal (MGF same)

-   ï¼ˆ1-n-1/N-1ï¼‰ times Var(p)

# Why n-1 in sampling variance

ï¼Ÿ

-   degree of freedom

-   ubiased estimation. (æ˜¯æŒ‡åœ¨å·²çŸ¥sampleæ—¶ï¼Œè¿™ä¸ªæ— åä¼°è®¡æ˜¯æ‰€æœ‰ä¼°è®¡ä¸­çš„å¹³å‡å€¼ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½ç§°å…¶ä¸ºâ€œæ— åâ€ã€‚)

![](images/clipboard-2819513011.png)

-   sampling proportion is unbiased

E($\hat P$) = E(X/n)=npn (binomial distribution) =p Var---also å¦‚æžœEæ»¡è¶³ variance ä¸æ»¡è¶³å‘¢

# Simple random samlple

We know the total population but do not know p of our interests.

Based on hypergeometric probability calculation we can estimate p as $\hat p = \text{intrests}/N= X_1+...+X_N/N$ where $X_i$\~ Bernoulli(p) and $\hat p$ is a random variable. Therefore, $E[\hat p] = p$ (E$X_i$=p based on Bernouli distribution).

-   If $X_i$ are independent then E($X_1+X_2$) = 2E($X_1$)

Here, eg. E$\hat p^2$= 1/n E$X_1^2$+n-1/nE$X_1 X_2$

Since $X_1=1$ $X_1^2=X_1$

and E$X_1 X_2$ = P$X_1, X_2$

Finally, we can look at the distribution of $\hat p$, suppose we know the true p is 0.54, we can use **simulation** to randomly sample $X_1,...X_n$ from Np people who support 1 and ......who support

-   God's perspective

```{r}
library(ggplot2)

set.seed(111)

population_size <- 12141897

p <- 0.54

num_simulations <- 50

sample_size <- 100

rep()
p_hat_values <- replicate(num_simulations, {
  # Simulate sampling from the population
  sample <- sample(c(rep(1,population_size * p), rep(0,population_size*(1-p))), sample_size, replace =FALSE) # replicate(num_simulations, {...})ï¼šè¿™ä¸ªå‡½æ•°ä¼šé‡å¤æ‰§è¡Œå¤§æ‹¬å·ä¸­çš„ä»£ç num_simulationsæ¬¡ï¼Œå¹¶å°†æ¯æ¬¡çš„ç»“æžœå­˜å‚¨åœ¨ä¸€ä¸ªå‘é‡ä¸­ï¼› repåˆ›å»ºä¸€ä¸ªåŒ…å«population_size * pä¸ª1å’Œpopulation_size * (1-p)ä¸ª0çš„å‘é‡ï¼Œæ¨¡æ‹Ÿæ€»ä½“ã€‚
mean(sample) #calculate the p_hat for each sample
})


  
histogram <- ggplot(data.frame(p = p_hat_values), aes(x=p))+
  geom_histogram(binwidth = 0.01, fill ="blue", color = "black")+
  labs(title =" ",
       x = "p_hat",
       y= "Frequency")+
  theme_minimal()

print(histogram)

```

doing this 500 times,

based on the central limit theory

Statistical inference and Probability distribution

-   estimation

-   condifidence intervals

-   hypothesis testing

# Sample Space and Probability Measure

$\sigma$ algebra $F$ is a collection of satisfying :

full and null set

completerment

countably union

measurble we can find a function that takes the elements of F and output a real number

A probability measure is a mapping P:F--\> R satisfying the following 3 axioms:

countably for mutually exclusive events $A_1, A_2,...\in F$

# interpret the probability

-   frequentist view

-   Bayesian view

Borel set is a combination of open set in some spaces

``` r
library(ggplot2)
library(dplyr)
library(readr)
library(magrittr)

circ <- read_csv("Charm_City_Circulator_Ridership.csv")
## take just average ridership per day
avg = circ %>% 
  filter(type == "Average")
# keep non-missing data
avg = avg %>% 
  filter(!is.na(number))
```

# Moment Generating Functions

$M_x (t)=E(e^{tx})$

## case 1

$M_x$ may not exist. When it exists in a neighborhood of 0, using talor

$$e^{tx}=1+tX+(tX)^2/2+...$$

$$M_x(t)=1+t\mu+t^2 \mu/2+...$$

$\mu_j = E(X^i)$ is the j-th moment of X. Therefore,

$$E(X^i)=M^{(j)}(0)$$

eg. Then we could also get the variance by take the 2nd order derivatives

## case 2

$\int$

eg. normal

-   X\~N(0,1)

idea: try to write an integral of a certain distribution's pdf and we get the result of this integral as 1. here, we get the pdf of the N(t,1) and finally we get $e^{t^2/2}$

-   X\~N($\mu, \sigma^2$)

Then X=$\mu+\sigma Z$ where Z\~N(0,1)

$$M_x(t)=E[e^{tx}]=e^{\mu t} E[e^{\sigma t Z}]=e^{\mu t} M_Z(\sigma t)=e^{\mu t +\sigma^2 t^2/2}$$

### Gamma disteibution

the family of gamma distributions generalizes the family of exponential distributions. The gamma distribution with shape r and rate $\lambda$

# ANOVA

-   Though its name is called " variance", it is actually a comparison among **means**

## Assumptions

Variable type: ANOVA requires a mix of one continuous quantitative dependent variable (which corresponds to the measurements to which the question relates) and one qualitative independent variable (with at least 2 levels which will determine the groups to compare).

Independence: the data, collected from a representative and randomly selected portion of the total population, should be independent between groups and within each group. The assumption of independence is most often verified based on the design of the experiment and on the good control of experimental conditions rather than via a formal test.

(If observations between samples (forming the different groups to be compared) are dependent (for example, if three measurements have been collected on the same individuals as it is often the case in medical studies when measuring a metric (i) before, (ii) during and (iii) after a treatment), the **repeated measures ANOVA** should be preferred in order to take into account the dependency between the samples.)

Normality: (same idea as "sampling distribution")

-   QQ-plot on the residuals.

-   In the context of ANOVA, residuals correspond to the differences between the observed values and the mean of all values for that group.

-   In case of small samples, residuals should follow approximately a normal distribution. The normality assumption can be tested visually thanks to a histogram and a QQ-plot, and/or formally via a normality test such as the Shapiro-Wilk or Kolmogorov-Smirnov test.

-   If, even after a transformation of your data (e.g., logarithmic transformation, square root, Box-Cox, etc.), the residuals still do not follow approximately a normal distribution, the Kruskal-Wallis test can be applied. This non-parametric test, robust to non normal distributions, has the same goal than the ANOVAâ€”compare 3 or more groupsâ€”but it uses sample medians instead of sample means to compare groups.

Equality of variances

-   This assumption can be tested graphically (by comparing the dispersion in a boxplot or dotplot for instance), or more formally via the Leveneâ€™s test or Bartlettâ€™s test.

-   If the hypothesis of equal variances is rejected, another version of the ANOVA can be used: the Welch ANOVA

-   Note that the Welch ANOVA does not require homogeneity of the variances, but the distributions should still follow approximately a normal distribution.

Outliers

## practice

```{r}

library(palmerpenguins)
# dat <- penguins %>%
#   select(species, flipper_length_mm)
dat <- penguins[,c("species", "flipper_length_mm")]
str(dat)

summary(dat)

# see it firstly

library(ggplot2)

ggplot(dat) +
  aes(x = species, y = flipper_length_mm, color = species) +
  geom_violin(draw_quantiles=T) + geom_boxplot(width=0.3) + 
  theme_minimal() + theme(legend.position = "none")

# ?warning?

# â€œIs the length of the flippers different between the 3 species of penguins?â€.

# use hypothesis testing (ANOVA)






```

Be careful that the alternative hypothesis is not that all means are different. The opposite of all means being equal (ð»0 ) is that at least one mean is different from the others (ð»1 ).

```r
TukeyHSD(mod, which ="sex:species")

c(1,"a", TRUE)


```

# GLM 



essentials

![](images/clipboard-1008509295.png)

## random component (exponential family density)

$f(y;\theta;\phi)=\exp(\frac{y \theta-b(\theta)}{a(\phi)}+c(y,\phi))$




### eg of linear regression


g($\mu$)=$\mu$



### eg of logistic regression


